# Model specifications
fine_tuning_version: '1.0'  # String: Version identifier for the fine-tuning process
max_seq_length: 1024        # Integer: Maximum sequence length for model input
prompt: "instruction': here is a product title from a Iranian marketplace.  \n         give me the Product Entity and Attributes of this product in Persian language.\n         give the output in this json format: {'attributes': {'attribute_name' : <attribute value>, ...}, 'product_entity': '<product entity>'}.\n         Don't make assumptions about what values to plug into json. Just give Json not a single word more.\n         \nproduct title:"              # String: Identifier for the specific prompt template used
dataset_name_or_path: "BaSalam/entity-attribute-dataset-GPT-3.5-generated-v1"  # String: Path or identifier for the dataset
model_name_or_path: "NousResearch/Llama-2-7b-chat-hf"  # String: Model identifier or path from Hugging Face Hub

# Training configuration
project_name: "Llama2-7b-entity-attr"  # String: Project name for organization and output directories
dataloader_drop_last: True             # Boolean: Whether to drop the last incomplete batch
evaluation_strategy: "steps"           # String: Strategy to use for evaluating model performance
learning_rate: 0.00015                 # Float: Initial learning rate for the optimizer
per_device_train_batch_size: 16        # Integer: Training batch size per device
per_device_eval_batch_size: 16         # Integer: Evaluation batch size per device
gradient_accumulation_steps: 1         # Integer: Number of steps to accumulate gradients before updating model weights
save_steps: 0                          # Integer: Number of steps between saving the model
logging_steps: 30                      # Integer: Number of steps between logging training progress
warmup_steps: 100                      # Integer: Number of steps to perform learning rate warmup
num_train_epochs: 2                    # Integer: Total number of training epochs
weight_decay: 0.01                     # Float: Weight decay coefficient for regularization
eval_steps: 75000                      # Integer: Number of steps between evaluations
gradient_checkpointing: True           # Boolean: Whether to use gradient checkpointing to save memory
fp16: False                            # Boolean: Whether to use 16-bit floating-point precision training
bf16: False                            # Boolean: Whether to use bfloat16 precision training if supported
logging_strategy: "steps"              # String: Determines how logging is handled

# LoRA (Low-Rank Adaptation) specifications
lora_bias: "none"                      # String: Specific bias configuration for LoRA layers; "none" indicates no bias
lora_rank: 128                         # Integer: Rank for the low-rank matrices in LoRA layers
lora_task_type: "CAUSAL_LM"            # String: Task type for LoRA configuration
lora_dataloader_drop_last: True        # Boolean: Whether to drop the last incomplete batch for LoRA specific data
lora_target_modules: ["q_proj", "v_proj", 'k_proj']              # List: Specific modules to target for LoRA adaptation, None applies to all eligible
lora_dropout: 0.05                     # Float: Dropout rate for LoRA layers
lora_alpha: 256                        # Integer: Alpha parameter controlling the scale of LoRA adaptation

# BitsAndBytes specifications for quantization
load_in_4bit: True                     # Boolean: Whether to load the model using 4-bit quantization
bnb_4bit_quant_type: 'nf4'             # String: Type of 4-bit quantization
bnb_4bit_compute_dtype: 'float16'      # String: Compute data type when using 4-bit quantization
bnb_4bit_use_double_quant: False       # Boolean: Whether to use double quantization for increased precision

# Tokenizer specifications
response_template: " ### Answer:"      # String: Template for formatting the model's responses
user_prompt_template: "### Question: " # String: Template for formatting the input prompts
percent_of_train_dataset: 0.985        # Float: Percentage of the training dataset to use
padding_side: "right"                  # String: Which side of the sequence to pad
mlm: False                             # Boolean: Whether to use Masked Language Modeling (MLM)
